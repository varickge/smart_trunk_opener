{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7750729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fec615",
   "metadata": {},
   "source": [
    "#### GPU setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9de440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets limit on and selects index-th gpu\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "index = 0\n",
    "\n",
    "tf.config.experimental.set_memory_growth(gpus[index], True)\n",
    "# select the gpu you want to use\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[index], \"GPU\")\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf3251",
   "metadata": {},
   "source": [
    "### Data Preprocessing, Augmenting,  Loading and checking shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(root_path=\"Data\", is_aug=True, sample_count=10)\n",
    "train_data, test_data = data_loader()\n",
    "_, train_target, train_labels = train_data\n",
    "\n",
    "# ------- load test data from folders not from .npy -------\n",
    "# _, test_target, test_labels = data_loader.DataLoad(data_path=\"Data/benchmarking_intive/\", decode_target=True)\n",
    "_, test_target, test_labels = data_loader.DataLoad(\n",
    "    data_path=\"Data/test/\", decode_target=False\n",
    ")\n",
    "# ------- load test data from folders not from .npy -------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a09de38",
   "metadata": {},
   "source": [
    "### Creating model and checking summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_len = 50\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(split_len - 2, 8), batch_size=None),\n",
    "        tf.keras.layers.GRU(units=64, return_sequences=False),\n",
    "        tf.keras.layers.Dense(2),\n",
    "        tf.keras.layers.Softmax(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "weights_path = \"models/test.h5\"\n",
    "model.save_weights(weights_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a46dc",
   "metadata": {},
   "source": [
    "### Trainer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad410fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(\n",
    "    model, data, weights_path, data_test, batch_size=64, epochs=15, learning_rate=0.003\n",
    "):\n",
    "    X_train, Y_train = data\n",
    "\n",
    "    # Declaring training checkpoints (save weights --> best only)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        weights_path, monitor=\"val_loss\", verbose=0, save_best_only=True, mode=\"min\"\n",
    "    )\n",
    "    callbacks_list = [\n",
    "        checkpoint\n",
    "    ]  # , tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)]\n",
    "\n",
    "    # Start training,\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_data=data_test,\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7a39ca",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede98b93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "kicks = train_target[np.where(train_labels == 1)[0]]\n",
    "no_kicks = train_target[np.where(train_labels == 0)[0]]\n",
    "\n",
    "# this is for balanced training\n",
    "no_kick_splits = np.array_split(\n",
    "    no_kicks,\n",
    "    len(no_kicks) // len(kicks) + 1\n",
    "    if len(no_kicks) % len(kicks)\n",
    "    else len(no_kicks) / len(kicks),\n",
    ")\n",
    "\n",
    "# Compiling model, declaring Loss function (Sparse Categorical Crossentropy), optimizer (ADAM)\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"acc\"],\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        decay=0,\n",
    "        amsgrad=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "for no_kick_i in no_kick_splits:\n",
    "    train_data_i = np.concatenate([no_kick_i, kicks], axis=0)\n",
    "    train_labels_i = np.concatenate([np.zeros(len(no_kick_i)), np.ones(len(kicks))])\n",
    "    history = trainer(\n",
    "        model=model,\n",
    "        epochs=10,\n",
    "        data=(train_data_i, train_labels_i),\n",
    "        data_test=(test_target, test_labels),\n",
    "        batch_size=64,\n",
    "        weights_path=weights_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4715508d",
   "metadata": {},
   "source": [
    "### Checking Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ba9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loss in per epoch\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101baf8",
   "metadata": {},
   "source": [
    "### Model evaluation on test (unseen, noaugmented) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('models/our_train_our_fit_full.h5')\n",
    "# _, test_data, test_labels = data_loader.DataLoad(data_path=\"Data/benchmarking_intive\", decode_target=True)\n",
    "# test_data, test_labels = data_loader.TestDataLoad()\n",
    "_, test_data, test_labels = data_loader.DataLoad(\n",
    "    data_path=\"Data/test/\", decode_target=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50972f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23905ab",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">ACC</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data, verbose=0)\n",
    "\n",
    "# Calculate ACC\n",
    "print(\"\\033[1m TEST ACC -->\", CalcACC(pred, test_labels), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248effd8",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Kick ACC</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698baed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kick = model.predict(test_data[np.where(test_labels == 1)[0]], verbose=0)\n",
    "\n",
    "# Calculate ACC\n",
    "print(\n",
    "    \"\\033[1m TEST KICK ACC -->\",\n",
    "    CalcACC(pred_kick, np.ones(len(test_data[np.where(test_labels == 1)[0]]))),\n",
    "    \"%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d8820",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">No kick ACC</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_no_kick = model.predict(test_data[np.where(test_labels == 0)[0]], verbose=0)\n",
    "\n",
    "# Calculate ACC\n",
    "print(\n",
    "    \"\\033[1m TEST NO KICK ACC -->\",\n",
    "    CalcACC(pred_no_kick, np.zeros(len(test_data[np.where(test_labels == 0)[0]]))),\n",
    "    \"%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f571a13",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Precision and recall</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bdf9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(test_labels, pred.argmax(axis=-1)).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "print(\"\\033[1m TEST PRECISION -->\", np.round(precision * 100, 2), \"%\")\n",
    "print(\"\\033[1m TEST RECALL -->\", np.round(recall * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846c5c5",
   "metadata": {},
   "source": [
    "|  ACC   | Kick Acc | No kick Acc  |   P   |   R   |\n",
    "|:------:|:--------:|:------------:|:-----:|:-----:|\n",
    "| 98.17  |  86.5    |    99.66     | 96.98 | 86.5  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
